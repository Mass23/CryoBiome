"""
Author: Susheel Bhanu BUSI
Affiliation: ESB group LCSB UniLU
Date: [2021-01-31]
Run: snakemake -s extraction_Snakefile --use-conda --cores 45 -rp
Latest modification:
"""

import os, fnmatch
import glob
import numpy as np
import pandas as pd
from collections import Counter

configfile:"extraction_config.yaml"
DATA_DIR=config['data_dir']
RESULTS_DIR=config['results_dir']
KFASTA_DIR=config['kfasta_dir']
SAMPLES=[line.strip() for line in open("cluster_list", 'r')]    # if using a sample list instead of putting them in a config file
ACCESSION=[line.strip() for line in open("sample_list", 'r')]

###########
rule all:
    input:
        os.path.join(DATA_DIR, "clusters_min_30_seq_2_samp_SB.tsv"),
        expand(os.path.join(RESULTS_DIR, "fasta/Cluster_{sample}.fa"), sample=SAMPLES),
        expand(os.path.join(RESULTS_DIR, "contigs/Cluster_{sample}_contigs.txt"), sample=SAMPLES),
        os.path.join(RESULTS_DIR, "coverage/merged_euci_coverages.txt"),
        expand(os.path.join(RESULTS_DIR, "coverage/Cluster_{sample}_coverages.txt"), sample=SAMPLES),
        expand(os.path.join(RESULTS_DIR, "tRNA/Cluster_{sample}_tRNA.txt"), sample=SAMPLES),
        expand(os.path.join(RESULTS_DIR, "Stats/Cluster_{sample}.txt"), sample=SAMPLES),
        expand(os.path.join(RESULTS_DIR, "emboss/Cluster_{sample}_consensus.fa"), sample=SAMPLES),
#        expand(os.path.join(RESULTS_DIR, "diamond/Cluster_{sample}.tsv"), sample=SAMPLES),
        os.path.join(RESULTS_DIR, "diamond/merged_Cluster.tsv"),
        expand(os.path.join(RESULTS_DIR, "Stats/Cluster_{sample}_len_GC_PID.txt"), sample=SAMPLES),
        expand(os.path.join(RESULTS_DIR, "Pairwise_identity/Cluster_{sample}.txt"), sample=SAMPLES)

################################
# rules for files and analyses #
################################
rule cluster_gather:
    input:
        linclust=os.path.join(KFASTA_DIR, "results/mmseqs2/clusters_linclust.tsv"),
        metadata=os.path.join(DATA_DIR, "MTG_metadata.tsv")
    output:
        os.path.join(DATA_DIR, "clusters_min_30_seq_2_samp_SB.tsv")
    log:
        os.path.join(RESULTS_DIR, "logs/linclust_gather.log")
    message:
        "Extracting cluster information from MMSEQS2 LINCLUST output"
    run:
        clusters_mmseqs = pd.read_csv(snakemake.input.linclust, sep='\t', header = None)
        print(len(clusters_mmseqs))
        print(len(set(clusters_mmseqs[0])))

        # subset clusters that are present in at least 2 samples, and have 30 sequences
        from collections import Counter
        count_dict = Counter(clusters_mmseqs[0])
        
        #subset more than 29 sequences
        count_dict = { k:v for k, v in count_dict.items() if v > 29}
        clusters_mmseqs = clusters_mmseqs[clusters_mmseqs[0].isin(list(count_dict.keys()))]
        print(len(clusters_mmseqs))
        print(len(set(clusters_mmseqs[0])))

        clusters_mmseqs.rename(columns = {0:'Centroid',1:'Sequence'}, inplace=True)
        print(clusters_mmseqs.columns)

        clusters_subset = pd.DataFrame(columns=['ClusterID','Centroid','Sequence'])
        metadata = pd.read_csv(snakemake.input.metadata, sep='\t')
        EUCI_samples = metadata[metadata['Cryosphere'] == 'Yes']['Sample'].to_list()

        # Counting clusters with sequences
        count_kept = 0
        count_removed = 0
        for cluster in list(set(clusters_mmseqs['Centroid'])):
            samples = list(set([i.split('_')[1] for i in clusters_mmseqs.loc[clusters_mmseqs['Centroid'] == cluster,'Sequence']]))
            if len(samples) > 1:
                cluster_data = clusters_mmseqs[clusters_mmseqs['Centroid'] == cluster]
                cluster_data['ClusterID'] = 'Cluster' + str(count_kept)
                clusters_subset = clusters_subset.append(cluster_data, ignore_index=True)
                count_kept += 1
            else:
                count_removed += 1
            s = f"""
            ------------------
            Kept : {count_kept}
            Removed : {count_removed}
            ------------------"""
            print(s)

        print(clusters_subset)
        clusters_subset.to_csv(snakemake.output[0],sep='\t')

rule fasta_extraction:
    input:
        fasta=os.path.join(KFASTA_DIR, "results/concat/concatenated_KEGG.fasta"),
        cluster=os.path.join(DATA_DIR, "Cluster_{sample}.txt")
    output:
        os.path.join(RESULTS_DIR, "fasta/Cluster_{sample}.fa")
    log:
        os.path.join(RESULTS_DIR, "logs/{sample}.fasta.log")
    conda:
        os.path.join("envs/bbmap.yaml")
    message:
        "Extracting fasta sequence for {wildcards.sample}"
    shell:
        "(date && filterbyname.sh in={input.fasta} out={output} include=t names={input.cluster} substring=f && date)"

rule contigs:
    input:
        rules.fasta_extraction.input.cluster
    output:
        os.path.join(RESULTS_DIR, "contigs/Cluster_{sample}_contigs.txt")
    log:
        os.path.join(RESULTS_DIR, "logs/{sample}.contigs.log")
    message:
        "Extracting original contig list for {wildcards.sample}"
    run:
        df=pd.read_csv(input[0], sep="\t", header=None).assign(clusterID=os.path.basename(input[0]))  # read file and add filename as columns
        df["clusterID"] = df["clusterID"].str.replace(".txt", "")   # replacing string
        df.rename(columns={0:'kegg_contig'}, inplace=True)
        df['contig']=df['kegg_contig']
#        df["contig"] = df["contig"].str.replace("Unassigned_", "").str.replace("_", "_contig_")
        df["contig"] = df["contig"].str.replace("Unassigned_", "").str.split('_').str[-2:].str.join('_contig_')
        df = df.reindex(['clusterID','kegg_contig','contig'], axis=1)
        df.to_csv(output[0], sep="\t", index=False)

rule concat:
    input:
        expand(os.path.join(DATA_DIR, "coverage/{accession}_covstats.txt"), accession=ACCESSION)
    output:
        os.path.join(RESULTS_DIR, "coverage/merged_euci_coverages.txt")
    log:
        os.path.join(RESULTS_DIR, "logs/concat.coverages.log")
    message:
        "Concatenating all coverage values for downstream analyses"
    shell:
        "(date && awk FNR-1 {input} > merged_tmp && "
        "echo -e 'contig\tcoverage' | cat - merged_tmp > out && mv out {output} && rm -rf merged_tmp && date) &> {log}"

rule coverages:
    input:
        contigs=rules.contigs.output,
        coverages=rules.concat.output
#        os.path.join(RESULTS_DIR, "coverage/merged_euci_coverages.txt")
    output:
        os.path.join(RESULTS_DIR, "coverage/Cluster_{sample}_coverages.txt")
    log:
        os.path.join(RESULTS_DIR, "logs/{sample}.merge_coverage.log")
    message:
        "Merging coverage for {wildcards.sample}"
    run:
        contigs=pd.read_csv(input[0], header=0, sep="\t")
        coverages=pd.read_csv(input[1], header=0, sep="\t")
        merged=pd.merge(contigs, coverages, on='contig')
        merged.to_csv(output[0], sep="\t", index=False)

#################
# tRNA analyses #
#################
rule tRNA:
    input:
        rules.fasta_extraction.output
    output:
        tab=os.path.join(RESULTS_DIR, "tRNA/Cluster_{sample}_tRNA.txt"),
        str=os.path.join(RESULTS_DIR, "tRNA/Cluster_{sample}_structure")
    conda:
        os.path.join("envs/trnascanse.yaml")
    log:
        os.path.join(RESULTS_DIR, "logs/{sample}.tRNA.log")
    message:
        "Running tRNA analysis for {wildcards.sample}"
    shell:
        "(date && tRNAscan-SE -G -o {output.tab} -f {output.str} {input} && date) &> {log}"


####################
# Cluster analyses #
####################
rule info:
    input:
        rules.fasta_extraction.output
    output:
        os.path.join(RESULTS_DIR, "Stats/Cluster_{sample}.txt")
    conda:
        os.path.join("envs/emboss.yaml")
    log:
        os.path.join(RESULTS_DIR, "logs/{sample}.stats.log")
    message:
        "Running basic Stats, i.e. length, GC content on {wildcards.sample}"
    shell:
        "(date && infoseq {input} -outfile {output} -name -length -pgc -auto -nousa && date) &> {log}"

rule mafft:
    input:
        rules.fasta_extraction.output
    output:
        os.path.join(RESULTS_DIR, "mafft/Cluster_{sample}.msf")
    conda:
        os.path.join("envs/mafft.yaml")
    log:
        os.path.join(RESULTS_DIR, "logs/{sample}.mafft.log")
    message:
        "Running multiple sequence alignment on {wildcards.sample}"
    shell:
        "(date && mafft {input} > {output} && date) &> {log}"

rule consensus:
    input:
        rules.mafft.output
    output:
        os.path.join(RESULTS_DIR, "emboss/Cluster_{sample}_consensus.fa")
    conda:
        os.path.join("envs/emboss.yaml")
    log:
        os.path.join(RESULTS_DIR, "logs/{sample}.emboss.log")
    message:
        "Generating consensus sequence for {wildcards.sample}"
    shell:
        "(date && cons -sequence {input} -outseq {output} -name Cluster_{wildcards.sample} -auto && date) &> {log}"

rule concat_fasta:
    input:
        expand(os.path.join(RESULTS_DIR, "emboss/Cluster_{sample}_consensus.fa"), sample=SAMPLES)
    output:
        os.path.join(RESULTS_DIR, "emboss/merged_Cluster.fa")
    log:
        os.path.join(RESULTS_DIR, "logs/merged_fasta.log")
    message:
        "Concatenating all fasta files from all Clusters for running diamond"
    shell:
        "(date && cat {input} > {output} && date) &> {log}"

rule uniprot:
    input:
        fa=rules.concat_fasta.output,
        db=config["diamond"]["db"]
    output:
        daa=os.path.join(RESULTS_DIR, "diamond/merged_Cluster.daa"), 
        tsv=os.path.join(RESULTS_DIR, "diamond/merged_Cluster.tsv")
    conda:
        os.path.join("envs/diamond.yaml")
    threads:
        config["diamond"]["threads"]
    params:
        outfmt="6 qseqid sseqid pident length mismatch gapopen qstart qend sstart send evalue bitscore qlen slen"
    log:
        os.path.join(RESULTS_DIR, "logs/merged_Cluster.uniprot.log")
    message:
        "Running UniProt trembl analyses for merged_Cluster fasta"
    shell:
        "(date && "
        "daa={output.daa} && "
        "diamond blastx -q {input.fa} --db {input.db} --out {output.daa} -p {threads} --outfmt 100 && "
        "diamond view --daa ${{daa%.*}} --max-target-seqs 1 -p {threads} --outfmt {params.outfmt} --out {output.tsv} && "
        "date) &> {log}"

rule identity:
    input:
        rules.mafft.output
    output:
        os.path.join(RESULTS_DIR, "Pairwise_identity/Cluster_{sample}.txt")
    conda:
        os.path.join("envs/clustal.yaml")
    log:
        os.path.join(RESULTS_DIR, "logs/{sample}.identity.log")
    message:
        "Calculating pairwise distances from MSA for {wildcards.sample}"
    shell:
        "(date && clustalo -i {input} --distmat-out={output} --full --percent-id && date) &> {log}"

#########
# Stats #
#########
rule stats:
    input:
        rules.info.output,
        rules.identity.output
    output:
        os.path.join(RESULTS_DIR, "Stats/Cluster_{sample}_len_GC_PID.txt")
    log:
        os.path.join(RESULTS_DIR, "logs/Cluster_{sample}.stats.log")
    message:
        "Estimating Mean, SD of sequence lengths, GC and PID (percent identity) for Cluster_{wildcards.sample}"
    run:
        # getting stats from infoseq output, i.e. length and GC
        df=pd.read_csv(input[0], header=0, delim_whitespace=True)
        stat=df.describe()
        stat=stat[['Length', '%GC']]

        # getting PID from MSA files
        pair=pd.read_csv(input[1], header=None, skiprows=1, delim_whitespace=True)
        stat['PID']=pair.describe()[1]
        
        # writing Stats to file
        stat.to_csv(output[0], sep='\t', index=True, header=True)
